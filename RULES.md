# VAND 2025 Challenge Rules

**Version:** 1.0 (Last Updated: 07-Apr-2025)

Please read these rules carefully before participating.

## 1. Introduction

- **Challenge Goal:** Briefly describe the main objective (e.g., develop few-shot anomaly detection models for industrial inspection).
- **Organizers:** You can find the list of organizers [here](https://sites.google.com/view/vand30cvpr2025/home)
- **Website:** https://sites.google.com/view/vand30cvpr2025/home
- **Timeline:** Apr 7th - May26th

## 2. Eligibility

- This challenge is open to individuals, teams, and academic and corporate entities worldwide.

## 3. Task Definition

- **Problem:** Participants will create models using few-shot learning and VLMs to find and localize structural and logical anomalies in the MVTec LOCO AD dataset, which contains images of different industrial products showing both defects. This indicates that the models can handle structural defect detection and logical reasoning.
- **Dataset:** [MVTec LOCO](https://www.mvtec.com/company/research/datasets/mvtec-loco) (specify categories used: `breakfast_box`, `juice_bottle`, `pushpins`, `screw_bag`, `splicing_connectors`). Link to the dataset page.

- **Input:**
  - The only inputs to the models are the k-shot support images and the test image.
- **Output:**
  - The models are expected to output a prediction score that is used to compute the Image F1Max score.
  - The models can optionally output a pixel-level anomaly map that is used to compute the pixel-level F1Max score.
- **Few-Shot Protocol:** Models will be tested with 1, 2, 4, and 8-shot samples for each of the seed values and categories.

## 4. Evaluation

- **Metrics:** The primary evaluation metric is the Image F1Max score. If the models are able to generate pixel-level anomaly maps, the pixel-level F1Max score is also computed.
- **Evaluation Server/Platform:** The code for the evaluation pipeline lives in `eval.py`, with the workflow defined in `.github/workflows/eval.yml`. The participants are not allowed to modify these files.
- **Leaderboard:** The leaderboard is autogenerated each time any participant/team makes a pull request that successfully runs the evaluation pipeline.

## 5. Submission Guidelines

- **Format:**
  - Code must be packaged within the provided `src/eval/submission/` directory.
  - Participants must implement the `Model` class in `model.py`.
- **Code Requirements:**
  - The submission system uses single Nvidia 3090 GPUs and 166GB of RAM per runner. The submission is expected to run within these constraints.
  - The runtime is expected to meet the cutoff limit. Any submission that does not meet the cutoff limit will automatically be terminated.
- **Submission Process:**
  - The participants are expected to make a pull request to the repository with their submission. The submission process is outlined in the [README](README.md).
  - The participants are free to update their PRs until the submission deadline.
- **Number of Submissions:** Currently, there is no limit on how many times a participant or team can submit. However, if we detect excessive computational resource usage by any participant/team, we may implement submission limits to ensure fair access for all.

## 6. Allowed Resources & Pre-training

- **External Data:** The participants are allowed to use any external data for pre-training. However, pre-training on the MVTec LOCO dataset is NOT allowed.
- **Pre-trained Models:** Pre-trained models should be available publically for download.
- **Training Data Usage:** Participants can only use the provided k-shot samples during `model.setup` for training. These images are used for pre-conditioning during inference, and not used for training.

## 7. Code Release & Reproducibility

- The entire evaluation pipeline is publically available on GitHub.
- All submissions will only be allowed as a publically accessible pull request to the repository.

## 8. General Rules & Conduct

- To make the evaluation fair and transparent, we will use the same evaluation protocol for all participants. This is done publically using GitHub Actions with the evaluation script available for scrutiny.
- All participants must use the same evaluation protocol and metrics for their submissions.
- Any team attempting to modify the evaluation script of the actions workflow will be disqualified.

## 9. Contact & Support

- For questions regarding the rules or evaluation, please contact the organizing team mentioned on the challenge [website](https://sites.google.com/view/vand30cvpr2025/home).

## 10. Amendments

- The organizers reserve the right to amend the rules, with notification to participants. This might include changes to the dataset, evaluation metrics, or submission guidelines.
- The organizers reserve the right to change the evaluation server/platform, include the cut-off time for submissions if needed.
